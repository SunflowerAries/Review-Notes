# <center>One-Way Functions are Essential for Non-Trivial Zero-Knowledge</center>

[TOC]

## Results

- **Theorem**: If uniform one-way functions exist then ZK = IP
- **Theorem 1**: If auxiliary-input one-way functions do not exist then ZK = BPP
- **Theorem 2**: If uniform one-way functions do not exist then ZK = AVBPP

## Required results for proving

The distributions M(x) generated by machines in PPT are called sampleable.

### BPP

$$
\{0, 1\}^n \supseteq L \in BPP \iff \exists M \in PPT \ s.t. \forall x \in \sum^* Pr[M(x) = L(x)] \ge \frac{3}{5}
$$

### AVBPP

Let $D_N$ be a sampleable ensemble on $\{0, 1\}^*$ with $D^n$ distributed over $\{0, 1\}^n$. $(L, D) \in AVBPP $  if  $\exist M \in PPT \ s.t. \ \forall n \in N, Pr[M(x) = L(x)] \ge \frac{3}{5} $, where $x \in D^n$

### One-Way Functions

$\textcolor{red} {Why \ to \ make \ such \ assumption} $

- **($\exist S1WF$)** There exists strong one-way functions $\triangleq \exist M \in PPT \ s.t. \forall N \in PPT,  \ |\sum^* - H_N(M)| < \infty$
- **($\exist 1WF$)** There exists one-way functions $\triangleq \exist M \in PPT \ s.t. \forall N \in PPT, \ |H_N(M)| = \infty$
- **($\nexists 1WF$) **There are no one-way functions $\triangleq \forall M \in PPT, \exist N \in PPT, \ |H_N(M)| < \infty$

### Universal Extrapolation and Approximation

- **(Universal Extrapolation)** $\triangleq$ For every $M \in PPT \ s.t. \exist N \in PPT$ satisfying for all $x \in \sum^*$,

$$
\left\| (M_1(x), M_2(x)) - (M_1(x), N(x, M_1(x), \alpha)) \right\|_1 \le \alpha
$$

- **(Universal Approximation)**$\triangleq$ For every $M \in PPT \ s.t. \exist N \in PPT$ satisfying for all $x \in \sum^*$,

$$
Prob[(1 - \alpha)|M_2|_{x, M_1(x)} < N(x, M_1(x), \alpha) < (1 + \alpha)|M_2|_{x, M_1(x)}] \ge 1 - \alpha
$$

- **Theorem 3**:

$$
\nexists 1WF \Rightarrow UE
$$

- **Theorem 4**:
  $$
  \nexists 1WF \Rightarrow UA
  $$

### Computationally Indistinguishable Distributions

- **Theorem 5**: Assuming $\nexists 1WF$, if $M, N \in PPT$, then
  $$
  (M \mathop =^c N) \Rightarrow (M \mathop =^s N)
  $$
  

### Interactive Proofs

- **Theorem 6**:
  $$
  IP = PSPACE
  $$

### Main Results

- **Theorem 1**
  $$
  \nexists 1WF \ over \sum = \{0, 1\} \Rightarrow (ZKHV = BPP)
  $$

- **Theorem 2**
  $$
  \nexists1WF \ over \sum = \{0\} \Rightarrow (AVZKHV = AVBPP)
  $$
  

## Basic Theorems

### D, E, F denote arbitrary ensembles

- A1:
  $$
  (D = E) \Rightarrow (D \mathop =^s E) \Rightarrow (D \mathop =^c E)
  $$

- A2:
  $$
  (D_1, D_2 \mathop =^c E_1, E_2) \Rightarrow (D_1 \mathop =^c E_1) \\
  (D_1, D_2 \mathop =^s E_1, E_2) \Rightarrow (D_1 \mathop =^s E_1)
  $$

- A3: All three relations are polynomially transitive (with additive bounds on distinguishing
  probability; namely transitivity can be used for distributions indexed by $x \in \sum^*$ only $|x|^{O(1)}$times

$\textcolor{red} {Why \ A3 \ ?}$

### For M $\in$ PTM

- A4: 
  $$
  (D \mathop =^s E) \Rightarrow (D, M(D) \mathop =^s E, M(E))
  $$

- A5:
  $$
  (D, E \mathop =^s F, M(F)) \Rightarrow (D, E \mathop =^s D, M(D))
  $$

- A6:
  $$
  (D \mathop =^c E) \Rightarrow (D, M(D) \mathop =^c E, M(E))
  $$

- A7:
  $$
  (D, E \mathop =^c F, M(F)) \Rightarrow (D, E \mathop =^c D, M(D))
  $$

### Under assumption $\nexists 1WF$

- B1:
  $$
  (M = M_1, M_2) \Rightarrow \exist N \in PPT \ s.t. \ (M \mathop =^s M_1, N(M_1))
  $$

- B2:
  $$
  (M \mathop =^c N) \Rightarrow (M \mathop =^s N)
  $$

- B3: Let D, E be an arbitrary distribution. For any e, let D, e be conditional distribution on D given that the second component of D, E is fixed to e, and let $T_e \triangleq \{d | d, e \ is \ in \ the \ support \ set \ of \ D, e \}$. For a fixed machine $N \in PPT$ consider the distribution N(E), E (generated by picking en element e according to E for the second component and applying N(.) to it to obtain the first component). $T_e^N \triangleq \{d | d, e \ is \ in \ the \ support \ set \ of \ N(e), e \}$. Then, the following implication holds:
  $$
  D, E \mathop =^s N(E), E \Leftarrow \begin{cases}
  (a) \ D, E \mathop =^c N(E), E
  \\
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ and
  \\ 
  (b) For \ any \ e, T_e^N \subseteq T_e, and \\
  D, e \ is \ uniform \ on \ T_e 
  \end{cases}
  $$

## Proof

### B3

The proof is by contradiction. Assume (b) and $D, E \ne^s N(E), E$ hold, and show that (a) does not hold. Let $\widehat N(w, e)$ be a deterministic analog of N(e). Fix some $e \in E$. For every d, let
$$
W_e^d \triangleq \{w | \widehat N(w, e) = d, e \}
$$
To prove that assuming (b) and $(D, E) \ne^s (N(E), E)$, there is a noticeable difference between the expected size $|W_e^d|$ when $d, e$ comes from $D, E$ versus $N(E), E$. Let $W_e \triangleq \cup_d W_e^d$, $U(e)$ denote a uniform distribution on $T_e$, and let N(e) be a distribution on $T_e^N$ which is computed according to a randomly chosen w s.t. $\widehat N(w, e) = d, e$.

- **Lemma 1** If $D, E \mathop \ne^s N(E), E$, then $\exists A \subseteq {E}$ and $\exists \alpha$ of size $\frac{1}{poly(w)}$ s.t.
  $$
  \sum_{e \in A} r(e) \ge \alpha \ and \ \forall e \in A \ \left\| U(e) - N(e) \right\|_1 \ge \alpha
  $$

- **Lemma 2** If $\nexists 1WF$ then there exists $M \in PPT$ such that on input $d, e$ and $0 < \beta < 1$, M
  outputs $p_d'$ and $t'$ such that
  $$
  P[(1 - \beta) p_d < p_d' < (1 + \beta)p_d] \ge 1 - \beta
  \\ and\\
  P[(1 - \beta)t < t' < (1 + \beta)t] \ge 1 - \beta
  $$
  Distinguisher operates as follows:

  - On input $d, e$ and $\alpha$ compute $p_d'$ and $t'$ within $(1 + \frac{\alpha^4}{8})$ of $p_d$ and $t$

  - compute $b = p_d' t'$

  - If $(b \ge 1)$

    ​	Then output 1
  
    Else 
    
    ​	output a coin flip which is biased toward 1 with probability b.
  
$$
  \delta_e \triangleq \sum_d(p_d \min(p_dt, 1) - q_d \min(p_dt, 1))
  $$
  
- **Lemma 3**
  $$
  \left\| N(e) - U(e) \right\|_1 = 2\gamma \Rightarrow \delta_e \ge \gamma_e^2
  $$
  